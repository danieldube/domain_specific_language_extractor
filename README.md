# Domain-Specific-Language-Extractor
This program extracts a domain specific language from C++ source code.

## Conventions
- Filenames use lowercase snake_case to keep includes predictable and tooling-friendly.

## DSL Representation
The analyzer emits its findings as a human-readable Markdown report (with an
optional JSON mirror) following the structure documented in
[`docs/dsl_representation.md`](docs/dsl_representation.md). The report
captures canonical terms, relationships between them, common workflows, and
incoherence warnings so developers can quickly understand and refine the
domain language used across the codebase.

## CLI usage
The CLI entrypoint is `dsl-extract`. The primary command is `analyze`, and it
defaults to that behavior when no subcommand is provided:

```
dsl-extract analyze --root <path> [--build <dir>] [--format markdown,json] \
  [--out <dir>] [--scope-notes <text>] [--config config.yaml] \
  [--log-level error|warn|info|debug] [--cache-ast] [--cache-dir <dir>] \
  [--clean-cache]
```

- `--config` loads YAML settings (CLI flags override file values). Supply
  `root`, `build`, `formats`, `cache_ast`, `cache_dir`, `clean_cache`, and
  `log_level` keys to mirror the CLI.
- Typed parsing uses `yaml-cpp` to avoid bespoke config parsers while keeping
  the reader isolated from the analysis core. Unknown keys fail fast with a
  clear error so configs stay explicit.
- `--log-level`, `--verbose`, or `--debug` control structured logging emitted
  per pipeline stage. Defaults to warnings only.
- `--extractor`, `--analyzer`, and `--reporter` let you pick a registered
  plug-in for each stage (defaults remain `heuristic`, `rule-based`, and
  `markdown`). The same keys can be set in the YAML config file.
- `--cache-ast` enables the AST cache keyed by toolchain/version and source
  hash; `--clean-cache` clears the cache before indexing, and
  `dsl-extract cache clean` removes the cache on demand.
- `--out` directs report outputs to a specific directory; omit it to keep the
  legacy behavior of writing under the analysis root.
- `--format` accepts a comma-separated list (supported: `markdown`, `json`),
  defaulting to Markdown only when unspecified.
- Exit codes: `0` when analysis finishes without coherence findings, `2` when
  incoherence findings are present, and `1` for fatal errors such as missing
  inputs or unsupported commands.

Regenerate reports from cached artifacts with the `report` command. It copies
existing `dsl_report.md`/`dsl_report.json` files into a new destination and can
filter which formats to emit:

```
dsl-extract report --root <path> [--out <dir>] [--format markdown,json]
```

- `--root` points to the directory containing cached reports from a previous
  `analyze` run.
- `--out` writes the regenerated reports to the provided directory; omit to
  overwrite the cached location.
- `--format` optionally restricts which cached formats to emit; when omitted,
  the command emits every cached format it finds under `--root`.

### Configuration file example (YAML)

YAML supports nested paths and list formats:

```
root: /project/root
build:
  directory: build-debug
out:
  path: reports
formats:
  - markdown
  - json
cache_ast: true
cache_dir: .dsl/cache
clean_cache: false
log_level: info
scope_notes: "Generated by CI"
extractor: heuristic
analyzer: rule-based
reporter: markdown
```

### Plug-in registry

`dsl::ComponentRegistry` holds factories for extractors, coherence analyzers,
and reporters. The global registry is pre-populated with the default
implementations, but you can register custom plug-ins at runtime:

```
auto registry = dsl::MakeComponentRegistryWithDefaults();
registry.RegisterExtractor("custom", [] {
  return std::make_unique<CustomExtractor>();
});
registry.RegisterAnalyzer("custom", [] {
  return std::make_unique<CustomAnalyzer>();
});
registry.RegisterReporter("custom", [] {
  return std::make_unique<CustomReporter>();
});

dsl::AnalyzerPipelineBuilder builder(registry);
builder.WithExtractorName("custom").WithAnalyzerName("custom");
```

CLI flags (`--extractor`, `--analyzer`, `--reporter`) and matching YAML keys
choose among the registered plug-ins. Omitting them keeps the defaults intact.

## Architecture Documentation
The Arc42 design document lives in [`docs/arc42.md`](docs/arc42.md). Consult it
before making significant changes so the architecture goals, scope, and
quality characteristics remain visible for every task.

## Pre-commit and test gate (mandatory)
All commits must pass the local checks before they land in the repository. The
expected flow is:

1. Install toolchain dependencies so the CMake configure step can find
   `libclang` and related LLVM tools.
2. Run the pre-commit hooks on the full tree.
3. Build and run the unit tests.

Skipping any of these steps causes the most common failures on CI (for example,
`cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Debug` will fail without the
`libclang-dev` headers installed). Use the platform-specific dependency script
before installing the hooks so the configure step always succeeds.

### Linux and macOS
1. Install system dependencies (clang/clang-tidy/clang-format, libclang
   headers, cmake, and ninja):

   ```bash
   ./scripts/install_dev_dependencies.sh
   ```

2. Create and activate a virtual environment:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. Install Python tooling and set up the git hook:

   ```bash
   pip install --upgrade pip pre-commit
   pre-commit install
   ```

4. Run the full suite on all files:

   ```bash
   pre-commit run --all-files
   ```

5. Build and execute the test suite:

   ```bash
   cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Debug
   cmake --build build -j
   ctest --test-dir build --output-on-failure -j
   ```

### Windows (PowerShell)
1. Install system dependencies:

   ```powershell
   ./scripts/install_dev_dependencies.ps1
   ```

2. Create and activate a virtual environment:

   ```powershell
   py -3 -m venv .venv
   .\.venv\Scripts\Activate.ps1
   ```

3. Install Python tooling and set up the git hook:

   ```powershell
   pip install --upgrade pip pre-commit
   pre-commit install
   ```

4. Run the full suite on all files:

   ```powershell
   pre-commit run --all-files
   ```

5. Build and execute the test suite:

   ```powershell
   cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Debug
   cmake --build build -j
   ctest --test-dir build --output-on-failure -j
   ```
